{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c36e7e3e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_downloader import DataDownloader\n",
    "from jwm_data_downloader import JwmDataDownloader\n",
    "\n",
    "load_dotenv()\n",
    "# Configuration\n",
    "FUNCTION_APP_URL = os.environ.get(\"MC_FUNCTION_APP_URL\")\n",
    "FUNCTION_CODE = os.environ.get(\"MC_FUNCTION_CODE\")\n",
    "downloader = DataDownloader(FUNCTION_APP_URL, FUNCTION_CODE)\n",
    "\n",
    "USERNAME = os.environ.get(\"JWM_USERNAME\")\n",
    "PASSWORD = os.environ.get(\"JWM_PASSWORD\")\n",
    "downloader_jwm = JwmDataDownloader(username=USERNAME, password=PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d614ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# available = downloader_jwm.list_available_files()\n",
    "# available\n",
    "# downloader_jwm.download_as_dataframe(\"utc/pk5y_actual_at_10-00.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0b3635",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Functions to convert to UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3688dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to add UTC timestamps based on date and hour columns\n",
    "def add_utc_25(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = \"date\",\n",
    "    hour_col: str = \"hour\",\n",
    "    tz: str = \"Europe/Warsaw\",\n",
    "    out_col: str = \"Date_utc\",\n",
    "    local_col: str = \"Date_cet\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Mapuje (data, godzina 0..22/23/24) -> lokalny timestamp w tz (start godziny),\n",
    "    a następnie konwertuje do UTC. Unika duplikatów w marcu i poprawnie rozróżnia\n",
    "    podwójną 02:00 w październiku.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    # Klucz do łączenia — północ danego dnia (bez strefy)\n",
    "    out[\"_date_key\"] = pd.to_datetime(out[date_col]).dt.normalize()\n",
    "    out[hour_col] = out[hour_col].astype(int)\n",
    "    maps = []\n",
    "    for d in out[\"_date_key\"].dropna().unique():\n",
    "        # północ lokalna na początku i końcu doby\n",
    "        start = pd.Timestamp(d).tz_localize(tz)\n",
    "        end = (pd.Timestamp(d) + pd.Timedelta(days=1)).tz_localize(tz)\n",
    "        # ciąg godzin tej doby w lokalnej strefie (długość 23/24/25)\n",
    "        rng = pd.date_range(start, end, freq=\"h\", inclusive=\"left\")\n",
    "        maps.append(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"_date_key\": d,\n",
    "                    hour_col: np.arange(0, len(rng), dtype=int),  # zmienione z 1 na 0\n",
    "                    local_col: rng,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    mapping = (\n",
    "        pd.concat(maps, ignore_index=True)\n",
    "        if maps\n",
    "        else pd.DataFrame(columns=[\"_date_key\", hour_col, local_col])\n",
    "    )\n",
    "    # Dołączamy lokalny czas; nieistniejące kombinacje dostaną NaT\n",
    "    out = out.merge(mapping, on=[\"_date_key\", hour_col], how=\"left\")\n",
    "    # Konwersja do UTC\n",
    "    out[out_col] = out[local_col].dt.tz_convert(\"UTC\")\n",
    "    # Porządki\n",
    "    out.drop(columns=[\"_date_key\"], inplace=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "#### function to add UTC timestamps based on date and hour columns, handling 2.5 hour for DST\n",
    "def add_utc_half(df, date_col=\"date\", hour_col=\"hour\"):\n",
    "    \"\"\"\n",
    "    Dodaje do DataFrame kolumny 'Date_cet' (czas lokalny Europe/Warsaw)\n",
    "    oraz 'Date_utc' (czas UTC). Obsługuje zmianę czasu (np. 2.0 = pierwsza 02:00,\n",
    "    2.5 = druga 02:00).\n",
    "    \"\"\"\n",
    "    # konwersja kolumny daty\n",
    "    base_date = pd.to_datetime(df[date_col])\n",
    "    whole_hour = np.floor(df[hour_col]).astype(int)\n",
    "\n",
    "    # przesunięcie dnia przy godzinie 24\n",
    "    date_shift = base_date + pd.to_timedelta((whole_hour == 24).astype(int), unit=\"D\")\n",
    "    whole_hour = np.where(whole_hour == 24, 0, whole_hour)\n",
    "\n",
    "    # budowa czasu lokalnego (naive, bez strefy)\n",
    "    naive_local = date_shift + pd.to_timedelta(whole_hour, unit=\"h\")\n",
    "\n",
    "    # rozróżnienie dwóch \"2:00\" w dniu zmiany czasu\n",
    "    ambiguous = np.where(\n",
    "        df[hour_col] == 2.0, True, np.where(df[hour_col] == 2.5, False, np.nan)\n",
    "    )\n",
    "\n",
    "    # nadanie strefy czasowej\n",
    "    df[\"Date_cet\"] = naive_local.dt.tz_localize(\"Europe/Warsaw\", ambiguous=ambiguous)\n",
    "\n",
    "    # konwersja do UTC\n",
    "    df[\"Date_utc\"] = df[\"Date_cet\"].dt.tz_convert(\"UTC\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993b9c6",
   "metadata": {},
   "source": [
    "# Baza danych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f29b3",
   "metadata": {},
   "source": [
    "## Plan koordynacyjny 5lat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35f6bf",
   "metadata": {},
   "source": [
    "### Baza MC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e414225",
   "metadata": {},
   "source": [
    "##### Old PSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "05ceb1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan Koordynacyjny Histoira\n",
    "pk_his = downloader.get_csv_as_dataframe(\n",
    "    \"power\", \"pse_plan_koordynacyjny_2021-2024.csv\"\n",
    ")\n",
    "# rename columns\n",
    "pk_his.rename(columns={\"Doba\": \"date\", \"Godzina\": \"hour\"}, inplace=True)\n",
    "pk_his[\"date\"] = pd.to_datetime(pk_his[\"date\"])\n",
    "# set index\n",
    "pk_his.set_index([\"date\"], inplace=True)\n",
    "\n",
    "# rename columns to pk_new names\n",
    "pk_his = pk_his.rename(\n",
    "    columns={\n",
    "        \"Moc dyspozycyjna JW i magazynów energii świadczących usługi bilansujące w ramach RB\": \"Moc dyspozycyjna JW i magazynow energii swiadczacych uslugi bilansujace w ramach RB\",\n",
    "        \"Moc dyspozycyjna JW i magazynów energii świadczących usługi bilansujące w ramach RB dostępna dla OSP\": \"Moc dyspozycyjna JW i magazynow energii swiadczacych uslugi bilansujace w ramach RB dostepna dla OSP\",\n",
    "        \"Obowiązki mocowe wszystkich jednostek rynku mocy\": \"Obowiazki mocowe wszystkich jednostek rynku mocy\",\n",
    "        \"Planowane saldo wymiany międzysystemowej\": \"Planowane saldo wymiany miedzysystemowej\",\n",
    "        \"Prognozowana generacja JW i magazynów energii nie świadczących usług bilansujących w ramach RB\": \"Prognozowana generacja JW i magazynow energii nie swiadczacych uslug bilansujacych w ramach RB\",\n",
    "        \"Prognozowana sumaryczna generacja źródeł fotowoltaicznych\": \"Prognozowana sumaryczna generacja zrodel fotowoltaicznych\",\n",
    "        \"Prognozowana sumaryczna generacja źródeł wiatrowych\": \"Prognozowana sumaryczna generacja zrodel wiatrowych\",\n",
    "        \"Prognozowana wielkość niedyspozycyjności wynikająca z ograniczeń sieciowych występujących w sieci przesyłowej oraz sieci dystrybucyjnej w zakresie dostarczania energii elektrycznej\": \"Prognozowana wielkosc niedyspozycyjnosci wynikajaca z ograniczen sieciowych wystepujacych w sieci przesylowej oraz sieci dystrybucyjnej w zakresie dostarczania energii elektrycznej\",\n",
    "        \"Przewidywana generacja zasobów wytwórczych nieobjętych obowiązkami mocowymi\": \"Przewidywana generacja zasobow wytworczych nieobjetych obowiazkami mocowymi\",\n",
    "        \"Przewidywana generacja JW i magazynów energii świadczących usługi bilansujące w ramach RB (3) - (10) - (13)\": \"Przewidywana generacja JW i magazynow energii swiadczacych uslugi bilansujace w ramach RB\",\n",
    "        \"Nadwyżka mocy dostępna dla OSP (8) + (10) - [(3)-(13)]-(14)\": \"Nadwyzka mocy dostepna dla OSP\",\n",
    "        \"Nadwyżka mocy dostępna dla OSP ponad wymaganą rezerwę moc (5) - (4)\": \"Nadwyzka mocy dostepna dla OSP ponad wymagana rezerwe mocy\",\n",
    "        \"Prognozowana wielkość niedyspozycyjności wynikających z warunków eksploatacyjnych JW świadczących usługi bilansujące w ramach RB\": \"Suma niedostepnosci (postoje + ubytki) ze wzgledu na warunki eksploatacyjne (WE)\",\n",
    "    }\n",
    ")\n",
    "# cut old data\n",
    "pk_his = pk_his.loc[:\"2024-06-14\"].iloc[:-23].copy()\n",
    "# hour index\n",
    "pk_his[\"hour_idx\"] = pk_his.groupby(\"date\").cumcount()\n",
    "# reset index\n",
    "pk_his.reset_index(inplace=True)\n",
    "\n",
    "## Append UTC timestamps\n",
    "pk_his = add_utc_25(\n",
    "    pk_his,\n",
    "    date_col=\"date\",\n",
    "    hour_col=\"hour_idx\",\n",
    "    tz=\"Europe/Warsaw\",\n",
    "    out_col=\"Date_utc\",\n",
    "    local_col=\"Date_cet\",\n",
    ")\n",
    "# hour column\n",
    "pk_his[\"hour\"] = pk_his[\"Date_cet\"].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39454df2",
   "metadata": {},
   "source": [
    "##### New PSE EOD from MC DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "acd2303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan Koordynacyjny Nowy\n",
    "pk_new = downloader.get_csv_as_dataframe(\"power\", \"pse_plan_koordynacyjny.csv\")\n",
    "# date and hour column\n",
    "pk_new[[\"date\", \"hour\"]] = pk_new[\"Doba\"].astype(str).str.split(\" \", expand=True)\n",
    "# crate column witch give numbers from 0 to 23/24/25 gruping on date\n",
    "pk_new[\"hour_idx\"] = pk_new.groupby(\"date\").cumcount()\n",
    "\n",
    "# Teraz użyj hour_idx jako hour_col\n",
    "pk_new = add_utc_25(\n",
    "    pk_new,\n",
    "    date_col=\"date\",\n",
    "    hour_col=\"hour_idx\",\n",
    "    tz=\"Europe/Warsaw\",\n",
    "    out_col=\"Date_utc\",\n",
    "    local_col=\"Date_cet\",\n",
    ")\n",
    "# hour column\n",
    "pk_new[\"hour\"] = pk_new[\"Date_cet\"].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14574141",
   "metadata": {},
   "source": [
    "##### PSE LIVE from MC DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fa065c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### download pk data live\n",
    "pk_live = downloader.get_csv_as_dataframe(\"power_live\", \"pse_plan_koordynacyjny.csv\")\n",
    "# replace spaces in column names\n",
    "pk_live.columns = pk_live.columns.str.replace(\" \", \"_\")\n",
    "# date and hour column\n",
    "pk_live[[\"date\", \"hour\"]] = pk_live[\"Doba\"].astype(str).str.split(\" \", expand=True)\n",
    "# hour index\n",
    "pk_live[\"hour_idx\"] = pk_live.groupby([\"date\", \"Data_aktualizacji\"]).cumcount()\n",
    "# add UTC timestamps\n",
    "pk_live = add_utc_25(\n",
    "    pk_live,\n",
    "    date_col=\"date\",\n",
    "    hour_col=\"hour_idx\",\n",
    "    tz=\"Europe/Warsaw\",\n",
    "    out_col=\"Date_utc\",\n",
    "    local_col=\"Date_cet\",\n",
    ")\n",
    "# hour column\n",
    "pk_live[\"hour\"] = pk_live[\"Date_cet\"].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5048716",
   "metadata": {},
   "source": [
    "###### Join all histroy of pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a7c51f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join dataframes\n",
    "pk = pd.concat([pk_his, pk_new])\n",
    "# replace spaces in column names\n",
    "pk.columns = pk.columns.str.replace(\" \", \"_\")\n",
    "# drop columns\n",
    "pk.drop(columns=[\"Doba\", \"Doba_handlowa\"], inplace=True)\n",
    "\n",
    "#### Non Linear History\n",
    "pk_mc_pl = pd.concat([pk, pk_live], axis=0)\n",
    "pk_mc_pl = pk_mc_pl.sort_values([\"Date_utc\"], ascending=True)\n",
    "#### Uppercase columns names\n",
    "pk_mc_pl.columns = [col[0].upper() + col[1:] if col else \"\" for col in pk_mc_pl.columns]\n",
    "\n",
    "# rename columns to english\n",
    "pk_mc = pk_mc_pl.rename(\n",
    "    columns={\n",
    "        \"Data_aktualizacji\": \"Date_of_update_cet\",\n",
    "        \"Data_publikacji\": \"Date_of_publication_cet\",\n",
    "        \"Planowane_saldo_wymiany_miedzysystemowej\": \"cross_border_balance_forecast\",\n",
    "        \"Moc_dyspozycyjna_JW_i_magazynow_energii_swiadczacych_uslugi_bilansujace_w_ramach_RB\": \"avail_cap_of_gen_unit_and_energy_storage\",\n",
    "        \"Moc_dyspozycyjna_JW_i_magazynow_energii_swiadczacych_uslugi_bilansujace_w_ramach_RB_dostepna_dla_OSP\": \"avail_cap_of_gen_unit_and_energy_storage_osp\",\n",
    "        \"Nadwyzka_mocy_dostepna_dla_OSP\": \"surplus_cap_avail_for_tso\",\n",
    "        \"Wymagana_rezerwa_mocy_OSP\": \"required_power_reserve\",\n",
    "        \"Przewidywana_generacja_zasobow_wytworczych_nieobjetych_obowiazkami_mocowymi\": \"pred_gen_by_res_not_covered_by_cap_market_obligation\",\n",
    "        \"Suma_niedostepnosci_(postoje_+_ubytki)_ze_wzgledu_na_warunki_eksploatacyjne_(WE)\": \"sum_of_planned_unavailability\",\n",
    "        \"Prognozowane_zapotrzebowanie_sieci\": \"grid_demand_forecast\",\n",
    "        \"Nadwyzka_mocy_dostepna_dla_OSP_ponad_wymagana_rezerwe_mocy\": \"surplus_cap_avail_for_tso_over_pow_res\",\n",
    "        \"Prognozowana_generacja_JW_i_magazynow_energii_nie_swiadczacych_uslug_bilansujacych_w_ramach_RB\": \"avail_gen_of_gen_unit_and_energy_storage_non_rb\",\n",
    "        \"Planowane_ograniczenia_dyspozycyjnosci_i_odstawien_MWE\": \"planned_restrictions\",\n",
    "        \"Prognozowana_sumaryczna_generacja_zrodel_wiatrowych\": \"wind_total_generation_forecast\",\n",
    "        \"Przewidywana_generacja_JW_i_magazynow_energii_swiadczacych_uslugi_bilansujace_w_ramach_RB\": \"avail_gen_of_gen_unit_and_energy_storage_rb\",\n",
    "        \"Prognozowana_sumaryczna_generacja_zrodel_fotowoltaicznych\": \"pv_total_generation_forecast\",\n",
    "        \"Obowiazki_mocowe_wszystkich_jednostek_rynku_mocy\": \"cap_market_obligation_of_all_cap_market_units\",\n",
    "        \"Prognozowana_wielkosc_niedyspozycyjnosci_wynikajaca_z_ograniczen_sieciowych_wystepujacych_w_sieci\"\n",
    "        \"_przesylowej_oraz_sieci_dystrybucyjnej_w_zakresie_dostarczania_energii_elektrycznej\": \"unavailability_forecast\",\n",
    "    }\n",
    ")\n",
    "# Normalize date columns\n",
    "pk_mc[\"Date_of_update_cet\"] = pd.to_datetime(pk_mc[\"Date_of_update_cet\"])\n",
    "pk_mc[\"Date_of_publication_cet\"] = pd.to_datetime(pk_mc[\"Date_of_publication_cet\"])\n",
    "# drop columns\n",
    "pk_mc.drop(\n",
    "    columns=[\"Data_utworzenia\", \"Date\", \"Hour\", \"Doba\", \"Doba_handlowa\", \"Hour_idx\"],\n",
    "    inplace=True,\n",
    ")\n",
    "# sort columns\n",
    "pk_mc = pk_mc.reindex(sorted(pk_mc.columns), axis=1)\n",
    "\n",
    "# Reset index to avoid duplicate label issues (as discussed earlier)\n",
    "pk_mc = pk_mc.reset_index(drop=True)\n",
    "\n",
    "# Compute the new datetime values and remove timezone before assignment\n",
    "new_values = (\n",
    "    pd.to_datetime(pk_mc[\"Date_cet\"]).dt.normalize()\n",
    "    + pd.to_timedelta(\"1 day\")  # Assuming the update time is 10:00 AM\n",
    ").dt.tz_localize(None)  # <-- This removes timezone info\n",
    "\n",
    "# Assign only to rows where Date_of_update_cet is NaT\n",
    "mask = pk_mc[\"Date_of_update_cet\"].isna()\n",
    "pk_mc.loc[mask, \"Date_of_update_cet\"] = new_values[mask]\n",
    "\n",
    "# Fill NaT values in Date_of_publication_cet with Date_of_update_cet\n",
    "pk_mc[\"Date_of_publication_cet\"] = pk_mc[\"Date_of_publication_cet\"].fillna(\n",
    "    pk_mc[\"Date_of_update_cet\"]\n",
    ")\n",
    "\n",
    "### set Date_of_publication_cet and Date_of_update_cet as CET timezone\n",
    "pk_mc[\"Date_of_update_cet\"] = pd.to_datetime(\n",
    "    pk_mc[\"Date_of_update_cet\"]\n",
    ").dt.tz_localize(\"Europe/Warsaw\", ambiguous=\"infer\", nonexistent=\"shift_forward\")\n",
    "pk_mc[\"Date_of_publication_cet\"] = pd.to_datetime(\n",
    "    pk_mc[\"Date_of_publication_cet\"]\n",
    ").dt.tz_localize(\"Europe/Warsaw\", ambiguous=\"infer\", nonexistent=\"shift_forward\")\n",
    "# sort values by Date_utc and Date_of_publication_cet\n",
    "pk_mc.sort_values(\n",
    "    by=[\"Date_utc\", \"Date_of_publication_cet\"], ascending=True, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a10b219",
   "metadata": {},
   "source": [
    "### Baza JWM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2b78ad",
   "metadata": {},
   "source": [
    "#### Saved at 10:00 history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "75c69b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pk5y saved at 10 from jwm\n",
    "pk5y_10 = downloader_jwm.download_as_dataframe(\"utc/pk5y_actual_at_10-00.csv\")\n",
    "# drop columns\n",
    "pk5y_10.drop(columns=[\"plan_day\", \"plan_indicator\", \"delivery_end\"], inplace=True)\n",
    "# rename columns\n",
    "pk5y_10 = pk5y_10.rename(\n",
    "    columns={\n",
    "        \"delivery_start\": \"Date_utc\",\n",
    "        \"publication_timestamp\": \"Date_of_publication_cet\",\n",
    "    }\n",
    ")\n",
    "# change to warsaw time\n",
    "pk5y_10[\"Date_cet\"] = pd.to_datetime(pk5y_10[\"Date_utc\"]).dt.tz_convert(\"Europe/Warsaw\")\n",
    "# creat 'Date_of_update_cet' as 'Date_cet' - one day and at 10:00\n",
    "pk5y_10[\"Date_of_update_cet\"] = pd.to_datetime(\n",
    "    pk5y_10[\"Date_cet\"] - pd.to_timedelta(\"1 day\")\n",
    ").dt.normalize() + pd.to_timedelta(10, unit=\"h\")\n",
    "# change 'Date_of_publication_cet' to same as 'Date_of_update_cet' because pse was giving no update time on utc column\n",
    "pk5y_10[\"Date_of_publication_cet\"] = pk5y_10[\"Date_of_update_cet\"]\n",
    "# date columns to datetime\n",
    "pk5y_10[\"Date_utc\"] = pd.to_datetime(pk5y_10[\"Date_utc\"])\n",
    "pk5y_10[\"Date_cet\"] = pd.to_datetime(pk5y_10[\"Date_cet\"])\n",
    "pk5y_10[\"Date_of_update_cet\"] = pd.to_datetime(pk5y_10[\"Date_of_update_cet\"])\n",
    "pk5y_10[\"Date_of_publication_cet\"] = pd.to_datetime(pk5y_10[\"Date_of_publication_cet\"])\n",
    "# sort columns\n",
    "pk5y_10 = pk5y_10.reindex(sorted(pk5y_10.columns), axis=1)\n",
    "# choose history till Date_cet = 2025-07-20\n",
    "pk5y_10 = pk5y_10[pk5y_10[\"Date_cet\"] < \"2025-07-20\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e0ccc7",
   "metadata": {},
   "source": [
    "#### Saved EOD history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f9c8b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pk5y saved eod from jwm\n",
    "pk5y_eod = downloader_jwm.download_as_dataframe(\"utc/pk5y_actual_eod.csv\")\n",
    "# drop columns\n",
    "pk5y_eod.drop(columns=[\"plan_day\", \"plan_indicator\", \"delivery_end\"], inplace=True)\n",
    "# rename columns\n",
    "pk5y_eod = pk5y_eod.rename(\n",
    "    columns={\n",
    "        \"delivery_start\": \"Date_utc\",\n",
    "        \"publication_timestamp\": \"Date_of_publication_cet\",\n",
    "    }\n",
    ")\n",
    "# change to warsaw time\n",
    "pk5y_eod[\"Date_cet\"] = pd.to_datetime(pk5y_eod[\"Date_utc\"]).dt.tz_convert(\n",
    "    \"Europe/Warsaw\"\n",
    ")\n",
    "# creat 'Date_of_update_cet' as 'Date_cet'\n",
    "pk5y_eod[\"Date_of_update_cet\"] = (\n",
    "    pd.to_datetime(pk5y_eod[\"Date_cet\"] - pd.to_timedelta(\"1 day\")).dt.normalize()\n",
    "    + pd.to_timedelta(23, unit=\"h\")\n",
    "    + pd.to_timedelta(59, unit=\"m\")\n",
    ")\n",
    "# change 'Date_of_publication_cet' to same as 'Date_of_update_cet' because pse was giving no update time on utc column\n",
    "pk5y_eod[\"Date_of_publication_cet\"] = pk5y_eod[\"Date_of_update_cet\"]\n",
    "# date columns to datetime\n",
    "pk5y_eod[\"Date_utc\"] = pd.to_datetime(pk5y_eod[\"Date_utc\"])\n",
    "pk5y_eod[\"Date_cet\"] = pd.to_datetime(pk5y_eod[\"Date_cet\"])\n",
    "pk5y_eod[\"Date_of_update_cet\"] = pd.to_datetime(pk5y_eod[\"Date_of_update_cet\"])\n",
    "pk5y_eod[\"Date_of_publication_cet\"] = pd.to_datetime(\n",
    "    pk5y_eod[\"Date_of_publication_cet\"]\n",
    ")\n",
    "# sort columns\n",
    "pk5y_eod = pk5y_eod.reindex(sorted(pk5y_eod.columns), axis=1)\n",
    "# to datetime\n",
    "pk5y_eod[\"Date_utc\"] = pd.to_datetime(pk5y_eod[\"Date_utc\"])\n",
    "# choose history till Date_cet = 2025-08-15\n",
    "pk5y_eod = pk5y_eod[pk5y_eod[\"Date_cet\"] < \"2025-08-15\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbca2b3",
   "metadata": {},
   "source": [
    "#### New pk5y on JWM base saved on 7:30, 10:05, 10:10, 10:15, 10:20, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb2ed28",
   "metadata": {},
   "source": [
    "##### pk5y saved at 7:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "26ebe147",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pk5y saved at 7:30\n",
    "pk5y_0730n = downloader_jwm.download_as_dataframe(\"utc/pk5y_forecast_07-30.csv\")\n",
    "# drop columns\n",
    "pk5y_0730n.drop(columns=[\"timeseries_plan_indicator\", \"delivery_end\"], inplace=True)\n",
    "# rename columns\n",
    "pk5y_0730n = pk5y_0730n.rename(\n",
    "    columns={\n",
    "        \"delivery_start\": \"Date_utc\",\n",
    "        \"publication_timestamp\": \"Date_of_publication_utc\",\n",
    "        \"timeseries_plan_created_date\": \"Date_of_update_utc\",\n",
    "    }\n",
    ")\n",
    "# if Date_of_publication_utc is na fill it with Date_of_update_utc\n",
    "pk5y_0730n[\"Date_of_publication_utc\"] = pk5y_0730n[\"Date_of_publication_utc\"].fillna(\n",
    "    pk5y_0730n[\"Date_of_update_utc\"]\n",
    ")\n",
    "# create 'Date_cet' from 'Date_utc'\n",
    "pk5y_0730n[\"Date_of_publication_cet\"] = pd.to_datetime(\n",
    "    pk5y_0730n[\"Date_of_publication_utc\"]\n",
    ").dt.tz_convert(\"Europe/Warsaw\")\n",
    "pk5y_0730n[\"Date_of_update_cet\"] = pd.to_datetime(\n",
    "    pk5y_0730n[\"Date_of_update_utc\"]\n",
    ").dt.tz_convert(\"Europe/Warsaw\")\n",
    "# drop not needed utc columns\n",
    "pk5y_0730n.drop(columns=[\"Date_of_publication_utc\", \"Date_of_update_utc\"], inplace=True)\n",
    "# create 'Date_cet' from 'Date_utc'\n",
    "pk5y_0730n[\"Date_cet\"] = pd.to_datetime(pk5y_0730n[\"Date_utc\"]).dt.tz_convert(\n",
    "    \"Europe/Warsaw\"\n",
    ")\n",
    "# sort columns\n",
    "pk5y_0730n = pk5y_0730n.reindex(sorted(pk5y_0730n.columns), axis=1)\n",
    "# to datetime\n",
    "pk5y_0730n[\"Date_utc\"] = pd.to_datetime(pk5y_0730n[\"Date_utc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6042da4",
   "metadata": {},
   "source": [
    "##### pk5y saved at 10:05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7e76f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pk5y saved at 10:05\n",
    "pk5y_1005n = downloader_jwm.download_as_dataframe(\"utc/pk5y_forecast_10-05.csv\")\n",
    "# drop columns\n",
    "pk5y_1005n.drop(columns=[\"timeseries_plan_indicator\", \"delivery_end\"], inplace=True)\n",
    "# rename columns\n",
    "pk5y_1005n = pk5y_1005n.rename(\n",
    "    columns={\n",
    "        \"delivery_start\": \"Date_utc\",\n",
    "        \"publication_timestamp\": \"Date_of_publication_utc\",\n",
    "        \"timeseries_plan_created_date\": \"Date_of_update_utc\",\n",
    "    }\n",
    ")\n",
    "# if Date_of_publication_utc is na fill it with Date_of_update_utc\n",
    "pk5y_1005n[\"Date_of_publication_utc\"] = pk5y_1005n[\"Date_of_publication_utc\"].fillna(\n",
    "    pk5y_1005n[\"Date_of_update_utc\"]\n",
    ")\n",
    "# create 'Date_cet' from 'Date_utc'\n",
    "pk5y_1005n[\"Date_of_publication_cet\"] = pd.to_datetime(\n",
    "    pk5y_1005n[\"Date_of_publication_utc\"]\n",
    ").dt.tz_convert(\"Europe/Warsaw\")\n",
    "pk5y_1005n[\"Date_of_update_cet\"] = pd.to_datetime(\n",
    "    pk5y_1005n[\"Date_of_update_utc\"]\n",
    ").dt.tz_convert(\"Europe/Warsaw\")\n",
    "# drop not needed utc columns\n",
    "pk5y_1005n.drop(columns=[\"Date_of_publication_utc\", \"Date_of_update_utc\"], inplace=True)\n",
    "# create 'Date_cet' from 'Date_utc'\n",
    "pk5y_1005n[\"Date_cet\"] = pd.to_datetime(pk5y_1005n[\"Date_utc\"]).dt.tz_convert(\n",
    "    \"Europe/Warsaw\"\n",
    ")\n",
    "# sort columns\n",
    "pk5y_1005n = pk5y_1005n.reindex(sorted(pk5y_1005n.columns), axis=1)\n",
    "# to datetime\n",
    "pk5y_1005n[\"Date_utc\"] = pd.to_datetime(pk5y_1005n[\"Date_utc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c1a793",
   "metadata": {},
   "source": [
    "##### pk5y saved at 10:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3f30fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pk5y saved at 10:10\n",
    "pk5y_1010n = downloader_jwm.download_as_dataframe(\"utc/pk5y_forecast_10-10.csv\")\n",
    "# drop columns\n",
    "pk5y_1010n.drop(columns=[\"timeseries_plan_indicator\", \"delivery_end\"], inplace=True)\n",
    "# rename columns\n",
    "pk5y_1010n = pk5y_1010n.rename(\n",
    "    columns={\n",
    "        \"delivery_start\": \"Date_utc\",\n",
    "        \"publication_timestamp\": \"Date_of_publication_utc\",\n",
    "        \"timeseries_plan_created_date\": \"Date_of_update_utc\",\n",
    "    }\n",
    ")\n",
    "# if Date_of_publication_utc is na fill it with Date_of_update_utc\n",
    "pk5y_1010n[\"Date_of_publication_utc\"] = pk5y_1010n[\"Date_of_publication_utc\"].fillna(\n",
    "    pk5y_1010n[\"Date_of_update_utc\"]\n",
    ")\n",
    "# create 'Date_cet' from 'Date_utc'\n",
    "pk5y_1010n[\"Date_of_publication_cet\"] = pd.to_datetime(\n",
    "    pk5y_1010n[\"Date_of_publication_utc\"]\n",
    ").dt.tz_convert(\"Europe/Warsaw\")\n",
    "pk5y_1010n[\"Date_of_update_cet\"] = pd.to_datetime(\n",
    "    pk5y_1010n[\"Date_of_update_utc\"]\n",
    ").dt.tz_convert(\"Europe/Warsaw\")\n",
    "# drop not needed utc columns\n",
    "pk5y_1010n.drop(columns=[\"Date_of_publication_utc\", \"Date_of_update_utc\"], inplace=True)\n",
    "# create 'Date_cet' from 'Date_utc'\n",
    "pk5y_1010n[\"Date_cet\"] = pd.to_datetime(pk5y_1010n[\"Date_utc\"]).dt.tz_convert(\n",
    "    \"Europe/Warsaw\"\n",
    ")\n",
    "# sort columns\n",
    "pk5y_1010n = pk5y_1010n.reindex(sorted(pk5y_1010n.columns), axis=1)\n",
    "# to datetime\n",
    "pk5y_1010n[\"Date_utc\"] = pd.to_datetime(pk5y_1010n[\"Date_utc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb2906",
   "metadata": {},
   "source": [
    "##### pk5y saved at 10:15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6e8fbdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pk5y saved at 10:15\n",
    "pk5y_1015n = downloader_jwm.download_as_dataframe(\"utc/pk5y_forecast_10-15.csv\")\n",
    "# drop columns\n",
    "pk5y_1015n.drop(columns=[\"timeseries_plan_indicator\", \"delivery_end\"], inplace=True)\n",
    "# rename columns\n",
    "pk5y_1015n = pk5y_1015n.rename(\n",
    "    columns={\n",
    "        \"delivery_start\": \"Date_utc\",\n",
    "        \"publication_timestamp\": \"Date_of_publication_utc\",\n",
    "        \"timeseries_plan_created_date\": \"Date_of_update_utc\",\n",
    "    }\n",
    ")\n",
    "# if Date_of_publication_utc is na fill it with Date_of_update_utc\n",
    "pk5y_1015n[\"Date_of_publication_utc\"] = pk5y_1015n[\"Date_of_publication_utc\"].fillna(\n",
    "    pk5y_1015n[\"Date_of_update_utc\"]\n",
    ")\n",
    "# create 'Date_cet' from 'Date_utc'\n",
    "pk5y_1015n[\"Date_of_publication_cet\"] = pd.to_datetime(\n",
    "    pk5y_1015n[\"Date_of_publication_utc\"]\n",
    ").dt.tz_convert(\"Europe/Warsaw\")\n",
    "pk5y_1015n[\"Date_of_update_cet\"] = pd.to_datetime(\n",
    "    pk5y_1015n[\"Date_of_update_utc\"]\n",
    ").dt.tz_convert(\"Europe/Warsaw\")\n",
    "# drop not needed utc columns\n",
    "pk5y_1015n.drop(columns=[\"Date_of_publication_utc\", \"Date_of_update_utc\"], inplace=True)\n",
    "# create 'Date_cet' from 'Date_utc'\n",
    "pk5y_1015n[\"Date_cet\"] = pd.to_datetime(pk5y_1015n[\"Date_utc\"]).dt.tz_convert(\n",
    "    \"Europe/Warsaw\"\n",
    ")\n",
    "# sort columns\n",
    "pk5y_1015n = pk5y_1015n.reindex(sorted(pk5y_1015n.columns), axis=1)\n",
    "# to datetime\n",
    "pk5y_1015n[\"Date_utc\"] = pd.to_datetime(pk5y_1015n[\"Date_utc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a6074b",
   "metadata": {},
   "source": [
    "##### pk5y saved at 10:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7cc9903d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "### pk5y saved at 10:20\n",
    "pk5y_1020n = downloader_jwm.download_as_dataframe(\"utc/pk5y_forecast_10-20.csv\")\n",
    "# drop columns\n",
    "pk5y_1020n.drop(columns=[\"timeseries_plan_indicator\", \"delivery_end\"], inplace=True)\n",
    "# rename columns\n",
    "pk5y_1020n = pk5y_1020n.rename(\n",
    "    columns={\n",
    "        \"delivery_start\": \"Date_utc\",\n",
    "        \"publication_timestamp\": \"Date_of_publication_utc\",\n",
    "        \"timeseries_plan_created_date\": \"Date_of_update_utc\",\n",
    "    }\n",
    ")\n",
    "# if Date_of_publication_utc is na fill it with Date_of_update_utc\n",
    "pk5y_1020n[\"Date_of_publication_utc\"] = pk5y_1020n[\"Date_of_publication_utc\"].fillna(\n",
    "    pk5y_1020n[\"Date_of_update_utc\"]\n",
    ")\n",
    "# create 'Date_cet' from 'Date_utc'\n",
    "pk5y_1020n[\"Date_of_publication_cet\"] = pd.to_datetime(\n",
    "    pk5y_1020n[\"Date_of_publication_utc\"]\n",
    ").dt.tz_convert(\"Europe/Warsaw\")\n",
    "pk5y_1020n[\"Date_of_update_cet\"] = pd.to_datetime(\n",
    "    pk5y_1020n[\"Date_of_update_utc\"]\n",
    ").dt.tz_convert(\"Europe/Warsaw\")\n",
    "# drop not needed utc columns\n",
    "pk5y_1020n.drop(columns=[\"Date_of_publication_utc\", \"Date_of_update_utc\"], inplace=True)\n",
    "# create 'Date_cet' from 'Date_utc'\n",
    "pk5y_1020n[\"Date_cet\"] = pd.to_datetime(pk5y_1020n[\"Date_utc\"]).dt.tz_convert(\n",
    "    \"Europe/Warsaw\"\n",
    ")\n",
    "# sort columns\n",
    "pk5y_1020n = pk5y_1020n.reindex(sorted(pk5y_1020n.columns), axis=1)\n",
    "# to datetime\n",
    "pk5y_1020n[\"Date_utc\"] = pd.to_datetime(pk5y_1020n[\"Date_utc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247b5e8",
   "metadata": {},
   "source": [
    "##### pk5y saved at 23:59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9acb7ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pk5y saved at 23:59\n",
    "pk5y_2359n = downloader_jwm.download_as_dataframe(\"utc/pk5y_forecast_23-59.csv\")\n",
    "# drop columns\n",
    "pk5y_2359n.drop(columns=[\"timeseries_plan_indicator\", \"delivery_end\"], inplace=True)\n",
    "# rename columns\n",
    "pk5y_2359n = pk5y_2359n.rename(\n",
    "    columns={\n",
    "        \"delivery_start\": \"Date_utc\",\n",
    "        \"publication_timestamp\": \"Date_of_publication_utc\",\n",
    "        \"timeseries_plan_created_date\": \"Date_of_update_utc\",\n",
    "    }\n",
    ")\n",
    "# if Date_of_publication_utc is na fill it with Date_of_update_utc\n",
    "pk5y_2359n[\"Date_of_publication_utc\"] = pk5y_2359n[\"Date_of_publication_utc\"].fillna(\n",
    "    pk5y_2359n[\"Date_of_update_utc\"]\n",
    ")\n",
    "# create 'Date_cet' from 'Date_utc'\n",
    "pk5y_2359n[\"Date_of_publication_cet\"] = pd.to_datetime(\n",
    "    pk5y_2359n[\"Date_of_publication_utc\"]\n",
    ").dt.tz_convert(\"Europe/Warsaw\")\n",
    "pk5y_2359n[\"Date_of_update_cet\"] = pd.to_datetime(\n",
    "    pk5y_2359n[\"Date_of_update_utc\"]\n",
    ").dt.tz_convert(\"Europe/Warsaw\")\n",
    "# drop not needed utc columns\n",
    "pk5y_2359n.drop(columns=[\"Date_of_publication_utc\", \"Date_of_update_utc\"], inplace=True)\n",
    "# create 'Date_cet' from 'Date_utc'\n",
    "pk5y_2359n[\"Date_cet\"] = pd.to_datetime(pk5y_2359n[\"Date_utc\"]).dt.tz_convert(\n",
    "    \"Europe/Warsaw\"\n",
    ")\n",
    "# sort columns\n",
    "pk5y_2359n = pk5y_2359n.reindex(sorted(pk5y_2359n.columns), axis=1)\n",
    "# to datetime\n",
    "pk5y_2359n[\"Date_utc\"] = pd.to_datetime(pk5y_2359n[\"Date_utc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16c57c",
   "metadata": {},
   "source": [
    "##### Join JWM DB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8e7642d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate pk5y_10 and pk5y_eod\n",
    "pk_jwm = pd.concat(\n",
    "    [\n",
    "        pk5y_10,\n",
    "        pk5y_eod,\n",
    "        pk5y_0730n,\n",
    "        pk5y_1005n,\n",
    "        pk5y_1010n,\n",
    "        pk5y_1015n,\n",
    "        pk5y_1020n,\n",
    "        pk5y_2359n,\n",
    "    ]\n",
    ")\n",
    "# sort values by date_utc and publication date\n",
    "pk_jwm = pk_jwm.sort_values(by=[\"Date_utc\", \"Date_of_publication_cet\"], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7691650b",
   "metadata": {},
   "source": [
    "### Join MC and JWM datam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "74fa1e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join pk_jwm with pk_mc on Date_utc\n",
    "pk5y_forecast = pd.concat([pk_mc, pk_jwm])\n",
    "# sort values by Date_utc and Date_of_publication_cet\n",
    "pk5y_forecast = pk5y_forecast.sort_values(\n",
    "    by=[\"Date_utc\", \"Date_of_publication_cet\"], ascending=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23654a6",
   "metadata": {},
   "source": [
    "# save to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "58152e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to parquet\n",
    "out_path = Path(__file__).parent / \"../out\"\n",
    "pk5y_forecast.to_parquet(\n",
    "    out_path / \"pk5y_forecast.parquet\",\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
